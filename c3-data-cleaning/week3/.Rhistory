dir()
data <- read.csv("Downloads/household_power_consumption.txt")
data[1,1]
data[1,2]
library(nlme)
library(lattice)
xyplot(weight ~ Time | Diet, BodyWeight)
library(lattice)
library(datasets)
data(airquality)
p <- xyplot(Ozone ~ Wind | factor(Month), data = airquality)
p
library(datasets)
data(airquality)
airquality = transform(airquality, Month = factor(Month))
qplot(Wind, Ozone, data = airquality, facets = . ~ Month)
qplot(Wind, Ozone, data = airquality, geom = "smooth")
library(ggplot2)
library(ggplot2movies)
g <- ggplot(movies, aes(votes, rating))
print(g)
install.packages("ggplot2")
library(ggplot2)
library(ggplot2movies)
g <- ggplot(movies, aes(votes, rating))
print(g)
install.packages("ggplot2movies")
library(ggplot2)
library(ggplot2movies)
g <- ggplot(movies, aes(votes, rating))
print(g)
qplot(votes, rating, data = movies)
qplot(votes, rating, data = movies, smooth = "loess")
qplot(votes, rating, data = movies) + geom_smooth()
qplot(votes, rating, data = movies) + geom_smooth()
mean <- 80
sd <- 10
DBP <- 70
pnorm(DBP, mean, sd)
mean <- 1100
sd <- 75
p <- 0.95
qnorm(p,mean,sd)
n <- 100
standard_dev <- 75
standard_error <- standard_dev/sqrt(100)
qnorm(.95,standard_error,mean=1100)
choose(5,4) * .5 ^ 5 + choose(5,5) * .5 ^ 5
pbinom(3,size=5,prob=.5,lower.tail = FALSE)
pbinom(3, size=5, prob=0.5, lower.tail=FALSE)
choose(5,4) * 0.5 ^ 5 + choose(5,5) * 0.5 ^ 5
choose(5,4)
choose(5,5)
RDI_mean <- 15
RDI_sd <- 10
RDI_n <- 100
RDI_sample_err <- 10/sqrt(100)
RDI_sample_err
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
lm(y ~ x)
fit<-lm(y~x)
est<-predict(fit,data.frame(x))
summary(fit)
x<-mtcars$wt
y<-mtcars$mpg
fit<-lm(y ~ x)
predict(fit,data.frame(x=mean(x)), interval="confidence")
predict(fit,data.frame(x=3), interval="prediction")
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
install.packages("AppliedPredictiveModeling")
data(AlzheimerDisease)
data(AlzheimerDisease)
library("AppliedPredictiveModeling")
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
testIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
training = adData[-testIndex,]
testing = adData[testIndex,]
testIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
testing = adData[-inTrain,]
createTimeSlices(1:9, 5, 1, fixedWindow = FALSE)
install.packages("caret")
library("caret")
adData = data.frame(diagnosis,predictors)
testIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
training = adData[-testIndex,]
testing = adData[testIndex,]
testIndex == -testIndex
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
names <- colnames(concrete)
names <- names[-length(names)]
featurePlot(x = training[, names], y = training$CompressiveStrength, plot = "pairs")
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
library(ggplot2)
library(caret)
ncol(training)
which(sapply(adData,class)=="factor")
summary(training$diagnosis)
training$diagnosis = as.numeric(training$diagnosis)
p <- prcomp(training[,grep('^IL',names(training))])
p$rotation[,1:7]
qplot(1:length(p$sdev),p$sdev / sum(p$sdev))
which(cumsum(p$sdev) / sum(p$sdev) <= .9)
(cumsum(p$sdev) / sum(p$sdev))[8]
preProc <- preProcess(training[,grep('^IL',names(training))],method="pca",thres=.9)
# See the result here
preProc
set.seed(13435)
X <- data.frame("var1" = sample(1:5), "var2"=sample(6:10), "var3"=sample(11:15))
X <- X[sample(1:5), ]; X$var2[c(1,3)] = NA
X
sample(1:%)
sample(1:5)
sample(1:5)
sample(1:5)
sample(1:5)
X
X[,1]
X[, "var1"]
X[1:2, "var2"]
X[(X$var1 <= & X$var3 > 11), ]
X[(X$var1 <= 3 & X$var3 > 11), ]
X[which(X$var2 > 8), ]
X[X$var2 > 8, ]
X[which(X$var2 > 8(), ]
sort(X$var1)
sort(X)
sort(X$var1, decreasing = T)
sort(X$var2, na.last = T)
X[order(X$var1),]
X[order(X$var2, na.last=T),]
dir
setwd("onlinecourse/coursera/datasciencecoursera/c3-data-cleaning")
dir.create("week3")
setwd("week3")
dir.create("data")
fileUrl <- "https://data.baltimorecity.gov/api/views/k5ry-ef3g/rows.csv?accessType=DOWNLOAD"
download.file(fileUrl, destfile="data/restaurants.csv", method="curl")
restData <- read.cvs("data/restaurants.csv")
dir()
dir("data")
restData <- read.csv("data/restaurants.csv")
head(restData)
head(restData, n=3)
tail(restData, n=2)
summary(restData)
str(restData)
quantile(restData$councilDistrict, na.rm=T)
quantile(restData$councilDistrict, na.rm=T, probs=c(0.5,0.75,0.9))
table(restData$zipCode, useNA="ifany")
sum(is.na(restData$councilDistrict))
any(is.na(restData$councilDistrict))
all(restData$zipCode > -1000000)
all(restData$zipCode > 0)
colSums(is.na(restData))
all(colSums(is.na(restData)) == 0)
table(restData$zipCode %in% c("21212"))
table(restData$zipCode %in% c("21212", "21213"))
restData$zipCode %in% c("21212", "21213")
restData[restData$zipCode %in% c("21212", "21213"), ]
data("UCBAdmissions")
DF = as.data.frame(UCBAdmissions)
summary(DF)
DF
xt <- xtabs(Freq ~ Gender + Admit, data=DF)
xt
restData
s1 <- seq(1,10, by=2)
s1
s1 <- seq(1,10, by = 2)
s1
s1 <- seq(1,10, length=3)
s1
s1 <- seq(1,10, length=100)
s1
restData$nearMe = restData$neighborhood %in% c("Roland Park", "Homeland")
table(restData$nearMe)
restData$zipWrong = ifelse(restData$zipCode < 0, T, F)
table(restData$zipWrong, restData$zipCode < 0)
restData$zipGroups = cut(restData$zipCode, breaks = quantile(restData$zipCode))
table(restData$zipGroups)
restData$zcf <- factor(restData$zipCode)
restData$zcf[1:10]
yesno <- sample(c(
"yes", "no"
), size=10, replace=T)
yesno
yesnofac = factor(yesno, levels=c("yes", "no"))
yesnofac
relevel(yesnofac, ref="yes")
as.numeric(yesnofac)
yesnofac = factor(yesno, levels=c("yes", "no"))
as.numeric(yesnofac)
head(mtcars)
fileUrl = "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
download.file(fileUrl, destfile="data/fdfss.csv", curl=T)
download.file(fileUrl, destfile="data/fdfss.csv", method="curl")
data <- read.csv("data/fdfss.csv")
str(data)
summary(data$ACR)
summary(data$AGS)
moreThan10ArcAnd10000plus <- data[data$ACR == 3 & data$AGS == 6, ]
which(moreThan10ArcAnd10000plus)
agricultureLogical <- data$ACR == 3 & data$AGS == 6
which(agricultureLogical)
head(which(agricultureLogical), n = 3)
library(jpeg)
install.packages("jpeg")
library(jpeg)
img <- readJPEG("data/getdata%2Fjeff.jpg", native=TRUE)
quantile(img, probs=c(0.3,0.8))
GDP <- read.csv("data/getdata%2Fdata%2FGDP.csv")
FEDSTATS <- read.csv("data/getdata%2Fdata%2FEDSTATS_Country.csv")
head(GDP)
head(FEDSTATS)
head(GDP)
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv"
f <- file.path(getwd(), "GDP.csv")
download.file(url, f)
dtGDP <- data.table(read.csv(f, skip = 4, nrows = 215))
dtGDP <- dtGDP[X != ""]
dtGDP <- dtGDP[, list(X, X.1, X.3, X.4)]
setnames(dtGDP, c("X", "X.1", "X.3", "X.4"), c("CountryCode", "rankingGDP",
"Long.Name", "gdp"))
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FEDSTATS_Country.csv"
f <- file.path(getwd(), "EDSTATS_Country.csv")
download.file(url, f)
dtEd <- data.table(read.csv(f))
dt <- merge(dtGDP, dtEd, all = TRUE, by = c("CountryCode"))
sum(!is.na(unique(dt$rankingGDP)))
